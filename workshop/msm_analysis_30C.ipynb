{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov State Models of Ising model dynamics\n",
    "\n",
    "Recall that the \"true\" activation rate at 30˚C (determined from 450 unbiased time traces of the Ising process) is\n",
    "\n",
    "$\\alpha_0 = 1.075 \\pm 0.008$ $\\text{kHz}$\n",
    "\n",
    "Here, we will work through the steps of building an MSM from a similar data set of 32 discrete-time trajectories, sampled every 1 µs.  For convenience, we have already simulated kinetic Monte Carlo traces, and converted them to discrete-time trajectories for you.\n",
    "\n",
    "How close do you think we can get to the true value of $\\alpha_0$?\n",
    "\n",
    "\n",
    "### Step 1:  Load in the trajectory data\n",
    "\n",
    "The trajectories are stored as $T \\times 400$ <code>numpy</code> arrays, where $T$ is the number of snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "### adjust the number of trajectories we want in our dataset by uncommenting\n",
    "tags = ['a','b','c','d','e','f','g','h']   # 8 trajectories\n",
    "#tags += ['i','j','k','l','m','n','o','p']  # 16\n",
    "#tags += ['q','r','s','t','u','v','w','x'] # 24\n",
    "#tags += ['aa','bb','cc','dd','ee','ff','gg','hh']   # 36\n",
    "\n",
    "filenames = ['data/%s_all.msmtraj.npy'%s for s in tags]\n",
    "nfiles = len(filenames)\n",
    "trajs = [np.load(filenames[trial]) for trial in range(nfiles)]\n",
    "\n",
    "print 'The shape of a trajectory array:', trajs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first of the trajectories..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skip_frames = 1\n",
    "dq = 1.0/(20*20)   # gating charge/ncells\n",
    "dt = 1e-6\n",
    "\n",
    "plt.figure( figsize=(20,4))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "# Let's make a time trace of the gating charge\n",
    "timepoints = np.arange(0,trajs[0].shape[0]-skip_frames)*dt  # in seconds\n",
    "gating_charge = dq*trajs[0][skip_frames:,:].sum(axis=1)\n",
    "plt.plot(timepoints*1000.0, gating_charge)   # convert to ms\n",
    "plt.xlim(0,250)\n",
    "plt.xlabel('time (ms)')\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "# Let's make a histogram of the gating charge\n",
    "qbins = np.arange(0.0, 1.0+2*dq, 2*dq)\n",
    "qcenters = (qbins[0:-1]+qbins[1:])/2.0\n",
    "counts, edges = np.histogram(gating_charge, bins=qbins)\n",
    "plt.plot(qcenters,counts)\n",
    "plt.xlabel('gating charge (e)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Perform time-lagged Indepedent Component Analysis (tICA)\n",
    "\n",
    "Next, the tICA algorithm is used perform dimensionality reduction to a subspace of tICs representing the degrees of freedom along which the most time-correlated (i.e. slowest) motions occur.\n",
    "\n",
    "This step requires constructing and diagonalizing a 400 $\\times$ 400 correlation matrix, so it may take some time.\n",
    "\n",
    "The output is:\n",
    "\n",
    "(1) A series of trajectories in the new coordinate system, i.e. the values of each tIC over time.  These trajectories are stored in <code>tica_coords</code>, and saved to file.\n",
    "\n",
    "(2) Instantiation and training of <code>tica</code>, an <code>sklearn</code>-style model object.  This object contains all the training data and model parameters, including the tICA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from msmbuilder.decomposition import tICA, PCA\n",
    "\n",
    "# Choose the number of tICA components used to project the data.\n",
    "ntica = 10\n",
    "\n",
    "print 'Computing tICA decomposition for ntica = %d ...'%ntica\n",
    "tica = tICA(n_components=ntica, lag_time=1)\n",
    "tica_coords = tica.fit_transform(trajs)\n",
    "\n",
    "for i in range(len(tica_coords)):\n",
    "    tag = tags[i]\n",
    "    filename = 'data/tica/%s_all.tica%d.npy'%(tag,ntica)\n",
    "    np.save(filename,tica_coords[i])\n",
    "    print '\\tWrote', filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first ten tICA components..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure( figsize=(20,8))\n",
    "\n",
    "npanels = 10\n",
    "for j in range(min(npanels,ntica)):\n",
    "    plt.subplot(2,5,j+1)\n",
    "    #print 'tica.eigenvectors_[:,j].min()', tica.eigenvectors_[:,j].min()\n",
    "    #print 'tica.eigenvectors_[:,j].max()', tica.eigenvectors_[:,j].max()\n",
    "    plt.pcolor( tica.eigenvectors_[:,j].reshape( (20,20) ), cmap='RdBu', vmin=-0.15, vmax=0.15)\n",
    "    if j == 0:\n",
    "        plt.colorbar()\n",
    "    plt.title('tICA $\\\\vec{e}_%d$'%(j+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize (some of) the projection of the trajectory data onto tIC$_1$ and tIC$_2$, the first two tICA components...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure( figsize=(16,8))\n",
    "skip_points = 1   #  adjust this if you want to subsample the data\n",
    "\n",
    "number_to_show = 2 #  adjust the number of trajectories to visualize\n",
    "for i in range(len(tica_coords[0:(number_to_show-1)])):  \n",
    "    plt.plot(tica_coords[i][::skip_points,0],tica_coords[i][::skip_points,1], linewidth=0.1)\n",
    "    plt.xlabel('tIC$_1$', fontsize=24)\n",
    "    plt.ylabel('tIC$_2$', fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clustering and visualization\n",
    "See: http://msmbuilder.org/3.7.0/examples/Fs-Peptide-in-RAM.html\n",
    "\n",
    "First, let's generate some initial points to start the *kmeans* clustering. This is important because random selection of cluster generators based on the samples alone will be biased toward the two basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the max and min of the tica data and scale the initial points accordingly\n",
    "tica_mins = np.zeros( (len(tica_coords),ntica) )\n",
    "tica_maxs = np.zeros( (len(tica_coords),ntica) ) \n",
    "for i in range(len(tica_coords)):\n",
    "    tica_mins[i,:] = np.min(tica_coords[i], axis=0)\n",
    "    tica_maxs[i,:] = np.max(tica_coords[i], axis=0)\n",
    "best_tica_mins = np.min(tica_mins, axis=0)\n",
    "best_tica_maxs = np.min(tica_maxs, axis=0)\n",
    "print 'best_tica_mins', best_tica_mins\n",
    "print 'best_tica_maxs', best_tica_maxs\n",
    "\n",
    "\n",
    "#######  Adjust this parameter to change the number of clusters.  \n",
    "#######  WARNING: Large numbers of clusters and large data sets will consume time and memory!\n",
    "number_of_clusters = 100 # 2000\n",
    "\n",
    "# NOTE: ntica is defined in the cells above\n",
    "initial_generators = np.random.random( (number_of_clusters,ntica) )\n",
    "widths = best_tica_maxs - best_tica_mins\n",
    "for i in range(ntica):\n",
    "    initial_generators[:,i] = initial_generators[:,i]*widths[i] + best_tica_mins[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering proceeds in two steps: (1) using a subset of the data to find the cluster generators, and (2) assigning the remaining data to the generators.  This saves time and memory, with (hopefully) minimally loss of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from msmbuilder.cluster import MiniBatchKMeans, KMeans\n",
    "#clusterer = MiniBatchKMeans(n_clusters=number_of_clusters, init=initial_generators)\n",
    "clusterer = KMeans(n_clusters=number_of_clusters, init=initial_generators)\n",
    "\n",
    "\n",
    "########################################################\n",
    "### 2) cluster using 1/100 of the data\n",
    "subset_stride = 100  #  Adjustable parameter\n",
    "\n",
    "# create a subset of the data to do initial cluster on\n",
    "tica_coords_subset = [t[0::subset_stride,:] for t in tica_coords]\n",
    "clusterer.fit(tica_coords_subset)\n",
    "\n",
    "# next, assign the rest of the data\n",
    "clustered_trajs = clusterer.predict(tica_coords)\n",
    "# print(clusterer.summarize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where the generators lie on the tIC1 tIC2 landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# join list of tICA trajectories into a single traj for plotting\n",
    "txx = np.concatenate(tica_coords)\n",
    "plt.hexbin(txx[:,0], txx[:,1], bins='log', mincnt=1, cmap='viridis')\n",
    "plt.scatter(clusterer.cluster_centers_[:,0],\n",
    "            clusterer.cluster_centers_[:,1], \n",
    "            s=100, c='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the higher-dimensions using tICA cross-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure( figsize=(16,16))\n",
    "npanels = 6\n",
    "\n",
    "for i in range(npanels):\n",
    "    for j in range(npanels):\n",
    "        if i < j:\n",
    "            plt.subplot(npanels,npanels, i+j*npanels + 1)\n",
    "            txx = np.concatenate(tica_coords)\n",
    "            plt.hexbin(txx[0::20,i], txx[0::20,j], bins='log', mincnt=1, cmap='viridis')\n",
    "            plt.scatter(clusterer.cluster_centers_[:,i], clusterer.cluster_centers_[:,j], s=50, c='w')\n",
    "            if j == npanels-1:\n",
    "                plt.xlabel('tIC$_%d$'%i, fontsize=16)\n",
    "            if i == 0:\n",
    "                plt.ylabel('tIC$_%d$'%j, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculation of implied timescales\n",
    "\n",
    "Finally, we calculate implied timescales with respect to lag time to find slowest rate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from msmbuilder.msm import ContinuousTimeMSM, MarkovStateModel, implied_timescales\n",
    "\n",
    "lag_times = np.array([1, 2, 4, 10, 15, 20, 30, 50, 75, 100, 200])  # in units of steps\n",
    "n_timescales = 10\n",
    "dt = 1.0e-6\n",
    "\n",
    "msm_timescales = implied_timescales(clustered_trajs, lag_times, n_timescales=n_timescales, msm=MarkovStateModel(verbose=False))\n",
    "for i in range(n_timescales):\n",
    "   plt.plot(lag_times*dt*1.0e6, msm_timescales[:, i]*dt*1.0e3, 'o-')  # lagtime in us,  implied timescales in ms\n",
    "\n",
    "plt.title('Discrete-time MSM Relaxation Timescales')\n",
    "plt.semilogy()\n",
    "plt.xlabel('lag time ($\\\\mu s$)')\n",
    "plt.ylabel('implied timescales (ms)')\n",
    "\n",
    "print '### spectral estimates ###'\n",
    "#lag_times = [1, 2, 4, 10, 15, 20, 30, 50, 75, 100, 200]\n",
    "tau = msm_timescales[list(lag_times).index(100), 0]\n",
    "print 'implied timescale (using 100 us lagtime):', tau*dt*1.0e3, 'ms'\n",
    "# Make a star on the plot indicating tau at 100 µs lagtime\n",
    "plt.plot(100,tau*dt*1.0e3,'y*', markersize=16)\n",
    "print 'average dwell time (ms), 1/k = 2*tau', 2*tau*dt*1.0e3\n",
    "print 'Spectral estimate of crossing rate,  k = (1/tau)/2:', 0.5/(tau*dt*1.0e3), 'kHz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questions\n",
    "\n",
    "How do the estimated rates change with\n",
    "* the number of states in the MSM?\n",
    "* the number of trajectories using to train the MSM?\n",
    "* the number of tICA components used in the projection?\n",
    "* the clustering method?\n",
    "\n",
    "### Further reading\n",
    "\n",
    "The above questions have to do with the problem of *model selection*.  Fortunately, there is a rigorous **variational principle** we can invoke to select optimal \"hyperparameters\" such as the number of states.\n",
    "\n",
    "\n",
    "**A variational approach to model selection.** Christof Schütte (ZIB) and Frank Noé (FUB) groups have shown that discretation of the conformational space leads to errors in approximating the eigenvectors of the (continuous) dynamical transfer operator, and the so-called Variational Approach to Conformational Dynamics (VAC), which says that eny error in the eigenvectors will always *underestimate* the implied timescales.  Therefore, as long as we can control for finite sampling and/or overfitting errors, the model with the *slowest* timescales is the optimal model.\n",
    "\n",
    "**Model selection in face of sampling errors**.  Using such a variational principle in practice can be difficult because of the presence of sampling noise.  McGibbon and Pande have a very nice method for dealing with this, by calculating the so-called Generalized matrix Raleigh quotient (GMRQ).  In order to avoid over-fitting to a finite sample of data, a cross-validation procedure can be performed in which the data is partioned into a number of separate training and testing trials.\n",
    "\n",
    "#### References\n",
    "\n",
    "Nüske, F., Keller, B. G., Perez-Hernandez, G., Mey, A. S. J. S., & Noé, F. (2014). Variational Approach to Molecular Kinetics. Journal of Chemical Theory and Computation, 10(4), 1739–1752. http://doi.org/10.1021/ct4009156\n",
    "\n",
    "Prinz, J.-H., Wu, H., Sarich, M., Keller, B., Senne, M., Held, M., et al. (2011). Markov models of molecular kinetics: generation and validation. The Journal of Chemical Physics, 134(17), 174105. http://doi.org/10.1063/1.3565032\n",
    "\n",
    "McGibbon, R. T., & Pande, V. S. (2015). Variational cross-validation of slow dynamical modes in molecular kinetics. The Journal of Chemical Physics, 142(12), 124105. http://doi.org/10.1063/1.4916292\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
